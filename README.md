
# Hand Gesture Recognition Using Leap Motion (Infrared Images)

This project implements a **deep learning-based hand gesture recognition system** utilizing grayscale infrared images captured from the **Leap Motion Controller**. The model classifies **10 distinct hand gestures in real-time** via a webcam, enabling intuitive **Human-Computer Interaction (HCI)**.

---

## ğŸ” Project Overview

- **ğŸ“¸ Input:** Grayscale infrared images of hands performing gestures  
- **ğŸ§  Model:** Convolutional Neural Network (CNN) developed using **TensorFlow/Keras**  
- **ğŸ¯ Output:** Real-time classification of gestures (e.g., *palm*, *L*, *fist*, *down*, etc.)  
- **ğŸ¥ Interface:** Real-time webcam feed with on-screen gesture predictions  

---

## ğŸ“ Dataset Information

- **Dataset:** Leap Motion Hand Gesture Dataset (Infrared)  
- **Source:** [LeapGestRecog â€“ Kaggle](https://www.kaggle.com/datasets/gti-upm/leapgestrecog)  
- **Structure:**
```

leapGestRecog/
â”œâ”€â”€ 00/
â”‚   â”œâ”€â”€ 01\_palm/
â”‚   â”œâ”€â”€ 02\_l/
â”‚   â”œâ”€â”€ ...
â”‚   â””â”€â”€ 09/
â”œâ”€â”€ 01/
â”œâ”€â”€ ...
â””â”€â”€ 09/

````
- **10 subjects** (`00`â€“`09`)  
- **10 gesture classes** (palm, L, fist, down, etc.)  
- **Format:** Grayscale `.png` images  

---

## ğŸ§  Model Training

Train the CNN model:
```bash
python train_gesture_model.py
````

* Saves trained model: `gesture_model.h5`
* Saves label encoder: `label_encoder.pkl`
* Generates accuracy plot: `training_accuracy.png`

---

## ğŸ¥ Real-Time Prediction

Run the webcam-based gesture recognition:

```bash
python predict.py
```

* Opens webcam feed
* Displays predicted gesture label in real time
* Press **ESC** to exit

---

## ğŸ“‚ File Structure

```
.
â”œâ”€â”€ gesture_model.h5          # Trained CNN model
â”œâ”€â”€ label_encoder.pkl         # Encoded class labels
â”œâ”€â”€ train_gesture_model.py    # Model training script
â”œâ”€â”€ predict.py                # Real-time prediction script
â”œâ”€â”€ training_accuracy.png     # Accuracy visualization
â”œâ”€â”€ README.md                 # Project documentation
```

---

## ğŸ“ˆ Performance

* **Validation Accuracy:** 99.96%
* **Overfitting:** No significant signs detected
* **Generalization:** Performs well across multiple subjects

---

## ğŸ“š Future Work

* Gesture-based control for media and applications
* Voice feedback for recognized gestures
* Model deployment using **Flask** or **Streamlit**

```


